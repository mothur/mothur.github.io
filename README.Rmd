
We need to get the mediawiki files from the wiki in mediawiki markup format

```{R}
library(tidyverse)
library(xml2)
library(rvest)

## Read page 1 of Special:AllPages
page1_url <- "https://mothur.org/wiki/Special:AllPages"
page1 <- xml2::read_html(page1_url)
page1_pages <- page1 %>% html_nodes("ul.mw-allpages-chunk") %>% html_nodes("li>:not(.mw-redirect)") %>% html_text()

## Read page 2 of Special:AllPages
page2_url <- page1 %>% html_nodes("a") %>% html_text() %>% str_subset(., "Next") %>% head(n=1) %>% str_replace(., "Next page \\((.*)\\)", "\\1") %>% paste0("https://mothur.org/w/index.php?title=Special:AllPages&from=", .)
page2 <- read_html(page2_url)
page2_pages <-  page2 %>% html_nodes("ul.mw-allpages-chunk") %>% html_nodes("li>:not(.mw-redirect)") %>% html_text()

## Confirm that there's no page 3 of Special:AllPages
page2 %>% html_nodes("a") %>% html_text() %>% str_detect(., "Next") %>% sum(.) == 0


## Page names, inserting an underscore for all spaces
pages <- c(page1_pages, page2_pages) %>% str_replace_all(., " ", "_") %>% str_replace_all(., "/", "%2F")


## Function to download raw mediawiki markup version of each page
download <- function(x){

	source <- paste0('https://www.mothur.org/w/index.php?title=', x, '&action=raw')
	destination <- paste0("_mw_files/", x, ".mw") %>%
			str_replace(., ":.*", ".mw") #remove : and following from filenames
	download.file(source, destination, quiet=TRUE)

}

unlink("_mw_files", recursive=TRUE)
dir.create("_mw_files", showWarnings=FALSE)
pages %>% map(., download)
```

There are a fair number of spam pages about viagra, gold, dresses, etc. that we'd like to remove

```{bash}
mkdir -p _helper_files
ls _mw_files/*mw > _helper_files/keep_files.txt

# manually edit file to remove spam pages

chmod -w _helper_files/keep_files.txt
```

Let's take those mw files that we downloaded, remove the spam, and convert them to markdown format.

```{R}
convert_mw_to_md <- function(mediawiki_file){

	#use pandoc to convert mediawiki markup to markdown
	markdown_file <- str_replace(mediawiki_file, "_mw_files/(.*).mw", "_wiki/\\1.md")
	system(paste0("pandoc -f mediawiki -t markdown ", mediawiki_file, " -o ", markdown_file))

	md <- scan(markdown_file, what=character(), quiet=TRUE, sep="\n", blank.lines.skip=FALSE)
	unlink(markdown_file)

	# convert second level headers denoted with underscores to pound symbols
	hyphen_lines <- str_which(md, "^---")
	if(length(hyphen_lines) > 0){
		md[hyphen_lines-1] <- paste0("## ", md[hyphen_lines-1])
		md <- md[-hyphen_lines]
	}

	# convert inline code into code blocks
	md <- str_replace(md, "[\\\\]+$", "")
	md <- str_replace(md, "`(.*)`$", "    \\1")

	#formatting for math
	md <- str_replace_all(md, "\\{\\{", "{\ {")
	md <- str_replace_all(md, "\\$", "$$")

	# get rid of weird wikilink tags
 	md <- str_replace_all(md, ' "wikilink"', '')

	# add yaml material
	title <- str_replace(markdown_file, "_wiki/(.*).md", "\\1") %>% str_replace_all(., "_", " ") %>% str_replace_all(., ":.*", "")

	if(str_detect(title, "\\.")){
		redirect <- paste0("redirect_from: '/wiki/", str_replace_all(title, " ", "_"), ".html'")
	} else {
		redirect <- paste0("redirect_from: '/wiki/", str_replace_all(title, " ", "_"), "'")
	}

	if(any(str_detect(md, "Category:Commands"))){

		md <- c(
							"---",
							paste0("title: '", tolower(title), "'"),
							paste0("tags: 'commands'"),
							redirect,
							"---",
							md
						)

		md <- str_replace(md, "\\[Category:Commands\\]\\(Category:Commands\\)", "")

	} else if(any(str_detect(md, "Category:FileTypes"))){

			md <- c(
								"---",
								paste0("title: '", tolower(title), "'"),
								paste0("tags: 'file types'"),
								redirect,
								"---",
								md
							)

			md <- str_replace(md, "\\[Category:FileTypes\\]\\(Category:Types\\)", "")

		} else {

		md <- c(
							"---",
							paste0("title: '", title, "'"),
							redirect,
							"---",
							md
						)
	}

	# remove the manually placed TOC and NOTOC
	md <- md[!str_detect(md, "\\\\_\\\\_TOC\\\\_\\\\_")]
	md <- md[!str_detect(md, "\\\\_\\\\_NOTOC\\\\_\\\\_")]

	# remove tags for section headers from the mw files
 	md <- str_replace(md, " \\{\\#.*\\}", "")

	# if a page has a self-link, make it bold instead
	stub <- str_replace_all(markdown_file, "_wiki/(.*).md", "\\1") %>% tolower()
	bold_stub <- paste0("**", stub, "**")
	link_stub <- paste0("\\[", stub, "\\]\\(", stub, "\\)")
	md <- str_replace_all(md, link_stub, bold_stub)
	md <- str_replace_all(md, paste0(" ", stub, " "), paste0(" ", bold_stub, " "))

	# make everything https
	md <- str_replace_all(md, "http://", "https://")

	# replace Media links with links to S3 mothur bucket
	md <- gsub(md, pattern="Media:(.*?)\\)", replacement="https://mothur.s3.us-east-2.amazonaws.com/wiki/\\L\\1)", perl=TRUE, ignore.case=TRUE)

	# fix naked links to wiki
	naked_wiki_links <- any(str_detect(md, "<(http.*?/wiki/.*?)>"))
	md <- str_replace_all(md, "<http.*?/wiki/(.*?)>", "[\\1](\\1)")
	md <- str_replace_all(md, "<(http.*?)>", "[\\1](\\1)")

	# remove hard links to wiki
	md <- str_replace_all(md, "https://.{0,4}mothur.org/wiki/", "")

	# make wiki links lowercase
	md <- gsub(md, perl=TRUE, pattern="\\[([^h][^t].*?)\\]", replacement="[\\L\\1]")


	# fix links to old forum
	md <- str_replace_all(md, "https://.{0,4}mothur.org/forum/", "https://forum.mothur.org/")

	# fix links to old version of blog
	md <- str_replace_all(md, "https://blog.mothur.org/(....)/../../", "/blog/\\1/")

	# remove stray : characters
	md <- str_replace_all(md, "  : ", "    ")

	# fix images
	md <- gsub(md, pattern="(!\\[.*?\\])\\(([^ ]*).*\\)", replacement="\\1(https://mothur.s3.us-east-2.amazonaws.com/wiki/\\L\\2)", perl=TRUE)
	md <- str_replace_all(md, "\\)\\{.*?\\}", ")")
	md <- str_replace(md, "^ +!", "!")

	# fix numbered lists
	md <- str_replace(md, "^(\\d*)\\.", "\n\\1\\\\.")

	output <- tolower(markdown_file)
	print(output)
	write(x=md, file=output)

	naked_wiki_links
}

unlink("_wiki/", recursive=TRUE)
dir.create("_wiki", showWarnings=FALSE)

conversion <- read_tsv("_helper_files/keep_files.txt", col_name=c("file")) %>%
	mutate(convert = map_lgl(file, convert_mw_to_md))
```





## To do...

```
#https://github.com/gjtorikian/html-proofer
htmlproofer --assume-extension --http-status-ignore "429"  ./_site > todo.txt
htmlproofer --assume-extension --check-html --enforce-https  ./_site > todo.txt
```

* once site is up...
	- Google tracking code - get new code from
	- google-site-verification - see what's in site_root
	- create redirects from blog.mothur.org to mothur.org/blog/
	- connect add comments


* long term
  - Set up CI
  - Make video showing people how to make changes
	- Tables need help
	- execution page
  - make separate pages for tags, create three/four column listing of tags
	- read.dist littered throughout wiki
	- cluster() littered throughout examples for calculators
  - align.report_file needs help
  - Jensen-Shannon Divergence
	- Square root Jensen-Shannon Divergence
	- shannonrange
	- citation page, general update of citations for different function (more for mothur than wiki)
	- get rid of extra files from mothur.s3
